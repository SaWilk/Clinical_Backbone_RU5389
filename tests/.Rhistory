xlsx_file              <- "2025-08-20_Remote-IDs_Projekt_8_Extended_SW.xlsx"
xlsx_sheet             <- "Combined"
# load data --------------------------------------------------------------------
dat_children_parents <- read.csv(file.path(in_path, file_children_parents),
sep = ";", stringsAsFactors = FALSE, check.names = FALSE)
# load mapping (Participant_ID, Remote_ID) from Excel ------------------------
map_ids_raw <- readxl::read_excel(file.path(info_path, xlsx_file),
sheet = xlsx_sheet)
# keep only needed columns, coerce to character, and standardize for safe joins
map_ids <- map_ids_raw |>
dplyr::select(Participant_ID, Remote_ID) |>
dplyr::mutate(
Participant_ID = as.character(Participant_ID),
Remote_ID      = as.character(Remote_ID),
Remote_ID_std  = tolower(trimws(Remote_ID))
)
# ensure remid/vpid exist; create if missing -----------------------------------
if (!"remid" %in% names(dat_children_parents)) {
dat_children_parents$remid <- NA_character_
}
if (!"vpid" %in% names(dat_children_parents)) {
dat_children_parents$vpid <- NA_character_
}
# standardize remid for joining ------------------------------------------------
dat_std <- dat_children_parents |>
dplyr::mutate(remid_std = tolower(trimws(as.character(remid))))
# join & translate remids -> vpid ----------------------------------------------
dat_joined <- dat_std |>
dplyr::left_join(map_ids, by = dplyr::join_by(remid_std == Remote_ID_std)) |>
dplyr::mutate(
vpid = ifelse(!is.na(remid) & nzchar(remid) & !is.na(Participant_ID),
Participant_ID,
vpid)
) |>
dplyr::select(-c(remid_std, Remote_ID_std, Remote_ID, Participant_ID))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#
# FOR: translate remids
# Author: Saskia Wilken (saskia.wilken@uni-hamburg.de)
# 2025-08-26
#
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# This script
# - reads the "children/parents" and "adults" questionnaire data
# - loads Remote ↔ Participant ID mappings from the Excel sheet "Combined"
# - fixes known data-entry issues before any checks
# - requires remid == remidcheck (except when remid == "XXXXX")
# - if remid == "XXXXX", uses remidcheck as the lookup key (via mapping), logs these rows
# - for valid rows, maps Remote_ID → Participant_ID, writes to vpid, and clears remid/remidcheck
# - saves the updated data as "..._remids_translated.csv"
# clean up R environment
rm(list = ls())
cat("\014")
# install & load required packages --------------------------------------------
pkg <- c("dplyr", "readxl", "rstudioapi")
to_install <- pkg[!sapply(pkg, require, character.only = TRUE)]
if (length(to_install)) install.packages(to_install, dependencies = TRUE)
invisible(lapply(pkg, library, character.only = TRUE))
# number display ---------------------------------------------------------------
options(scipen = 999)
# paths ------------------------------------------------------------------------
script_dir <- tryCatch(dirname(rstudioapi::getSourceEditorContext()$path),
error = function(e) getwd())
in_path      <- file.path(script_dir, "raw_data")
info_path    <- file.path(script_dir, "information")
# optional log dir for Project 8 special-case notes
log_dir <- file.path(script_dir, "01_project_data", "logs")
if (!dir.exists(log_dir)) dir.create(log_dir, recursive = TRUE, showWarnings = FALSE)
file_children_parents <- "results-survey798916.csv"
file_adults           <- "results-survey564757.csv"
out_children_parents  <- "results-survey798916_remids_translated.csv"
out_adults            <- "results-survey564757_remids_translated.csv"
xlsx_file   <- "2025-08-20_Remote-IDs_Projekt_8_Extended_SW.xlsx"
xlsx_sheet  <- "Combined"
# load data --------------------------------------------------------------------
dat_children_parents <- read.csv(file.path(in_path, file_children_parents),
sep = ";", stringsAsFactors = FALSE, check.names = FALSE)
dat_adults <- read.csv(file.path(in_path, file_adults),
sep = ";", stringsAsFactors = FALSE, check.names = FALSE)
# load mapping (Participant_ID, Remote_ID) from Excel --------------------------
map_ids <- readxl::read_excel(file.path(info_path, xlsx_file), sheet = xlsx_sheet) |>
dplyr::select(Participant_ID, Remote_ID) |>
dplyr::mutate(
Participant_ID = as.character(Participant_ID),
Remote_ID      = as.character(Remote_ID)
)
# fix known data-entry errors --------------------------------------------------
# 1) Extend mapping with special cases (all map to vpid 99999)
special_map <- data.frame(
Participant_ID = rep("99999", 8),
Remote_ID      = c("99", "999", "9999", "99999", "999999", "9999999", "R70999", "10"),
stringsAsFactors = FALSE
)
map_ids <- dplyr::bind_rows(map_ids, special_map) |>
dplyr::distinct(Remote_ID, .keep_all = TRUE)
# 2) Correct mis-typed remid (and remidcheck) in raw survey data (805199 → 80519)
fix_mistyped_both <- function(df) {
if (is.numeric(df$remid)) {
df$remid[df$remid == 805199] <- 80519
} else {
df$remid[df$remid == "805199"] <- "80519"
}
if ("remidcheck" %in% names(df)) {
if (is.numeric(df$remidcheck)) {
df$remidcheck[df$remidcheck == 805199] <- 80519
} else {
df$remidcheck[df$remidcheck == "805199"] <- "80519"
}
}
df
}
dat_children_parents <- fix_mistyped_both(dat_children_parents)
dat_adults           <- fix_mistyped_both(dat_adults)
# utilities --------------------------------------------------------------------
is_blank <- function(x) is.na(x) | x == ""
# Identity check before any mapping (except for the "XXXXX" rows) --------------
flag_and_report_mismatches <- function(df, dataset_name) {
remid_chr  <- as.character(df$remid)
check_chr  <- as.character(df$remidcheck)
xmask <- !is.na(remid_chr) & toupper(remid_chr) == "XXXXX"
both_present <- !is_blank(remid_chr) & !is_blank(check_chr)
mismatch <- both_present & (remid_chr != check_chr) & !xmask
if (any(mismatch)) {
idx <- which(mismatch)
cat("\n⚠️ [", dataset_name, "] remid vs remidcheck mismatch rows (", length(idx), "):\n", sep = "")
for (i in idx) {
cat("   ⚠️ row ", i, ": remid='", remid_chr[i], "', remidcheck='", check_chr[i], "' -> MANUAL CHECK NEEDED (skipped)\n", sep = "")
}
}
eligible <- !mismatch
eligible
}
# Unmapped check ---------------------------------------------------------------
find_unmapped <- function(df, remote_ids, eligible_rows) {
remid_chr  <- as.character(df$remid)
check_chr  <- as.character(df$remidcheck)
xmask      <- !is.na(remid_chr) & toupper(remid_chr) == "XXXXX"
key        <- ifelse(xmask & !is_blank(check_chr), check_chr, remid_chr)
use_mask   <- eligible_rows & !is_blank(key)
candidates <- key[use_mask]
is_numeric <- grepl("^[0-9]+$", candidates)
whitelist  <- candidates %in% c("R70999")
candidates <- candidates[is_numeric | whitelist]
setdiff(candidates, as.character(remote_ids))
}
# translate + log function -----------------------------------------------------
translate_and_log <- function(df, map_tbl, eligible_rows, dataset_name) {
remid_chr  <- as.character(df$remid)
check_chr  <- as.character(df$remidcheck)
xmask      <- !is.na(remid_chr) & toupper(remid_chr) == "XXXXX"
key <- ifelse(xmask & !is_blank(check_chr), check_chr, remid_chr)
df$key_for_mapping <- key
df <- df |>
dplyr::left_join(map_tbl, by = c("key_for_mapping" = "Remote_ID"))
can_translate <- eligible_rows & !is_blank(df$key_for_mapping) & !is.na(df$Participant_ID)
df$vpid[can_translate] <- df$Participant_ID[can_translate]
df$remid[can_translate]      <- NA
df$remidcheck[can_translate] <- NA
x_rows <- which(xmask & eligible_rows)
log_df <- NULL
if (length(x_rows) > 0) {
log_df <- data.frame(
dataset                = dataset_name,
row_index              = x_rows,
fallback_key_from_remidcheck = df$key_for_mapping[x_rows],
mapped_participant_ID  = df$Participant_ID[x_rows],
stringsAsFactors = FALSE
)
}
df <- dplyr::select(df, -key_for_mapping, -Participant_ID)
list(df = df, log = log_df)
}
# PROCESS: children/parents ----------------------------------------------------
eligible_cp <- flag_and_report_mismatches(dat_children_parents, "children/parents")
unmapped_children <- find_unmapped(dat_children_parents, map_ids$Remote_ID, eligible_cp)
if (length(unmapped_children) > 0) {
cat("\n⚠️ [children/parents] remids without mapping (n=", length(unmapped_children), "):\n", sep = "")
print(unmapped_children)
stop("⚠️ Unmapped remids detected (ignoring letter-only and using remidcheck when 'XXXXX'). Please update the Excel mapping and re-run.")
}
res_cp <- translate_and_log(dat_children_parents, map_ids, eligible_cp, "children_parents")
dat_children_parents_out <- res_cp$df
log_cp <- res_cp$log
# PROCESS: adults --------------------------------------------------------------
eligible_ad <- flag_and_report_mismatches(dat_adults, "adults")
unmapped_adults <- find_unmapped(dat_adults, map_ids$Remote_ID, eligible_ad)
if (length(unmapped_adults) > 0) {
cat("\n⚠️ [adults] remids without mapping (n=", length(unmapped_adults), "):\n", sep = "")
print(unmapped_adults)
stop("⚠️ Unmapped remids detected (ignoring letter-only and using remidcheck when 'XXXXX'). Please update the Excel mapping and re-run.")
}
res_ad <- translate_and_log(dat_adults, map_ids, eligible_ad, "adults")
dat_adults_out <- res_ad$df
log_ad <- res_ad$log
# Write Project 8 log for 'XXXXX' fallbacks ------------------------------------
log_all <- dplyr::bind_rows(log_cp, log_ad)
if (!is.null(log_all) && nrow(log_all) > 0) {
ts <- format(Sys.time(), "%Y%m%d_%H%M%S")
log_file <- file.path(log_dir, paste0("project8_xxxxx_fallback_log_", ts, ".csv"))
write.table(log_all, file = log_file, sep = ";", dec = ".", row.names = FALSE, qmethod = "double")
cat("\n⚠️ Project 8 'XXXXX' fallback log written to:\n", log_file, "\n")
}
# save results -----------------------------------------------------------------
write.table(dat_children_parents_out,
file = file.path(in_path, out_children_parents),
sep = ";", dec = ".", row.names = FALSE, qmethod = "double")
write.table(dat_adults_out,
file = file.path(in_path, out_adults),
sep = ";", dec = ".", row.names = FALSE, qmethod = "double")
cat("\n✅ Saved translated data to:\n",
file.path(in_path, out_children_parents), "\n",
file.path(in_path, out_adults), "\n", sep = "")
# Load necessary library
library(dplyr)
PATH_Q = "K:\Wilken Arbeitsordner\Clinical_Backbone_RU5389\01_project_data\3_backbone\questionnaires\3_2025-09-26_adults.xlsx";
PATH_Q = file.path("K:\Wilken Arbeitsordner\Clinical_Backbone_RU5389\01_project_data\3_backbone\questionnaires\3_2025-09-26_adults.xlsx");
dirname(rstudioapi::getSourceEditorContext()$path)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
PATH_Q = file.path("01_project_data", "3_backbone", "questionnaires", "3_2025-09-26_adults.xlsx");
PATH_Q
# Load necessary library
library(dplyr)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
PATH_Q = file.path("01_project_data", "3_backbone", "questionnaires", "3_2025-09-26_adults.xlsx");
# Read in the data (replace with your file path)
df <- read.csv("limesurvey_responses.csv", stringsAsFactors = FALSE)
# Load necessary library
library(dplyr)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
PATH_Q = file.path("01_project_data", "3_backbone", "questionnaires", "3_2025-09-26_adults.xlsx");
# Read in the data (replace with your file path)
df <- read.csv(PATH_Q, stringsAsFactors = FALSE)
# Load necessary library
library(dplyr)
library(readxl)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
PATH_Q = file.path("01_project_data", "3_backbone", "questionnaires", "3_2025-09-26_adults.xlsx");
# Read in the data (replace with your file path)
df <- read.excel(PATH_Q)
# Load necessary library
library(dplyr)
library(readxl)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
PATH_Q = file.path("01_project_data", "3_backbone", "questionnaires", "3_2025-09-26_adults.xlsx");
# Read in the data (replace with your file path)
df <- read_excel(PATH_Q)
PATH_Q = file.path(".", "01_project_data", "3_backbone", "questionnaires", "3_2025-09-26_adults.xlsx");
# Read in the data (replace with your file path)
df <- read_excel(PATH_Q)
PATH_Q = file.path("..", "01_project_data", "3_backbone", "questionnaires", "3_2025-09-26_adults.xlsx");
# Read in the data (replace with your file path)
df <- read_excel(PATH_Q)
# Load necessary library
library(dplyr)
library(readxl)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
PATH_Q = file.path("..", "01_project_data", "3_backbone", "questionnaires", "3_2025-09-26_adults.xlsx");
# Read in the data (replace with your file path)
df <- read_excel(PATH_Q)
# Convert start and submit dates to POSIXct (date-time objects)
df <- df %>%
mutate(
startdate = as.POSIXct(startdate, format = "%Y-%m-%d %H:%M:%S"),
submitdate = as.POSIXct(submitdate, format = "%Y-%m-%d %H:%M:%S")
)
# Calculate duration in minutes for each participant
df <- df %>%
mutate(duration_minutes = as.numeric(difftime(submitdate, startdate, units = "mins")))
# Compute the average duration (ignoring NAs)
average_duration <- mean(df$duration_minutes, na.rm = TRUE)
# Print result
cat("Average completion time:", round(average_duration, 2), "minutes\n")
### ---- PsyToolkit: average completion time (minutes) ----
# Read PsyToolkit survey export (adjust the path)
PATH_T = file.path("..", "01_project_data", "3_backbone", "experiment_data", "3_2025-09-26_adults.xlsx");
psy <-  read_excel(PATH_T)
to_posix <- function(x, fmt) suppressWarnings(as.POSIXct(x, format = fmt, tz = "UTC"))
# Helper: try a few common datetime formats
coalesce_time <- function(x) {
# try "YYYY-MM-DD HH:MM:SS", then "YYYY/MM/DD HH:MM:SS", then ISO
out <- to_posix(x, "%Y-%m-%d %H:%M:%S")
out[is.na(out)] <- to_posix(x[is.na(out)], "%Y/%m/%d %H:%M:%S")
out[is.na(out)] <- suppressWarnings(as.POSIXct(x[is.na(out)], tz="UTC"))
out
}
# Case 1: explicit duration column (seconds or ms)
if ("duration" %in% tolower(names(psy))) {
dcol <- psy[[ names(psy)[tolower(names(psy)) == "duration"][1] ]]
# guess unit: if mostly > 10,000 then it's likely milliseconds
unit_ms <- median(dcol, na.rm = TRUE) > 10000
mins <- if (unit_ms) dcol / 60000 else dcol / 60
avg_psy_minutes <- mean(mins, na.rm = TRUE)
} else {
# Build start and end POSIXct from available columns
# Try combined datetimes first
start <- NULL; end <- NULL
# common combined names
start_names <- tolower(names(psy)) %in% c("start", "starttime", "start_datetime", "startdate_time")
end_names   <- tolower(names(psy)) %in% c("end", "endtime", "end_datetime", "enddate_time", "submit")
if (any(start_names)) start <- coalesce_time(psy[[ which(start_names)[1] ]])
if (any(end_names))   end   <- coalesce_time(psy[[ which(end_names)[1] ]])
# If start comes split as startDate + startTime (PsyToolkit often provides these)
if (is.null(start) || all(is.na(start))) {
sn_date_idx <- which(tolower(names(psy)) == "startdate")
sn_time_idx <- which(tolower(names(psy)) == "starttime")
if (length(sn_date_idx) && length(sn_time_idx)) {
start <- coalesce_time(paste(psy[[sn_date_idx]], psy[[sn_time_idx]]))
}
}
# If end comes split as endDate + endTime
if (is.null(end) || all(is.na(end))) {
en_date_idx <- which(tolower(names(psy)) %in% c("enddate","submitdate"))
en_time_idx <- which(tolower(names(psy)) %in% c("endtime","submittime"))
if (length(en_date_idx) && length(en_time_idx)) {
end <- coalesce_time(paste(psy[[en_date_idx[1]]], psy[[en_time_idx[1]]]))
}
}
# Raw-file style "end" stamp like "2024-07-02-14-26" -> parse if present
if (!is.null(psy$end) && (is.null(end) || all(is.na(end)))) {
# convert "YYYY-MM-DD-HH-MM" -> "YYYY-MM-DD HH:MM"
end_str <- gsub("^([0-9]{4}-[0-9]{2}-[0-9]{2})-([0-9]{2})-([0-9]{2})$", "\\1 \\2:\\3:00", psy$end)
end <- to_posix(end_str, "%Y-%m-%d %H:%M:%S")
}
# Compute durations
mins <- as.numeric(difftime(end, start, units = "mins"))
avg_psy_minutes <- mean(mins, na.rm = TRUE)
}
cat("PsyToolkit – average completion time:", round(avg_psy_minutes, 2), "minutes\n")
unit_ms
# Load necessary library
install.packages(c("readxl","dplyr","lubridate","stringr"))  # if needed
# Load necessary library
install.packages(c("readxl","dplyr","lubridate","stringr"))  # if needed
library(readxl)
library(dplyr)
library(lubridate)
library(stringr)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
PATH_Q = file.path("..", "01_project_data", "3_backbone", "questionnaires", "3_2025-09-26_adults.xlsx");
# Read in the data (replace with your file path)
df <- read_excel(PATH_Q)
# Convert start and submit dates to POSIXct (date-time objects)
df <- df %>%
mutate(
startdate = as.POSIXct(startdate, format = "%Y-%m-%d %H:%M:%S"),
submitdate = as.POSIXct(submitdate, format = "%Y-%m-%d %H:%M:%S")
)
# Calculate duration in minutes for each participant
df <- df %>%
mutate(duration_minutes = as.numeric(difftime(submitdate, startdate, units = "mins")))
# Compute the average duration (ignoring NAs)
average_duration <- median(df$duration_minutes, na.rm = TRUE)
# Print result
cat("Average completion time:", round(average_duration, 2), "minutes\n")
### ---- PsyToolkit: average completion time (minutes) ----
# Read PsyToolkit survey export (adjust the path)
PATH_T = file.path("..", "01_project_data", "3_backbone", "experiment_data", "3_2025-09-26_adults.xlsx");
psy <-  read_excel(PATH_T)
# Parse TIME_start / TIME_end that may look like:
# "YYYY-MM-DD HH:MM:SS" or "DD.MM.YYYY HH:MM:SS" (common in EU exports)
parse_dt <- function(x) {
x <- str_trim(as.character(x))
suppressWarnings(parse_date_time(
x,
orders = c("Y-m-d H:M:S","Y-m-d H:M",
"d.m.Y H:M:S","d.m.Y H:M",
"Y/m/d H:M:S","Y/m/d H:M"),
tz = "UTC",
exact = FALSE
))
}
res <- psy %>%
mutate(
start_ts = parse_dt(.data$TIME_start),
end_ts   = parse_dt(.data$TIME_end),
duration_min_raw = as.numeric(difftime(end_ts, start_ts, units = "mins"))
) %>%
# basic quality control: drop negatives, ultra-short blips, and extreme outliers
mutate(duration_min = ifelse(duration_min_raw < 0 | duration_min_raw < 0.1 | duration_min_raw > 8*60,
NA_real_, duration_min_raw))
valid_n <- sum(!is.na(res$duration_min))
if (valid_n == 0) {
cat("No valid durations found. Check that TIME_start / TIME_end parse correctly.\n",
"Example values:\n",
head(psy[c('TIME_start','TIME_end')], 5), "\n")
} else {
avg <- mean(res$duration_min, na.rm = TRUE)
med <- median(res$duration_min, na.rm = TRUE)
cat(sprintf("PsyToolkit – average: %.2f min | median: %.2f min | n=%d valid of %d\n",
avg, med, valid_n, nrow(res)))
}
# Load necessary library
# install.packages(c("readxl","dplyr","lubridate","stringr"))  # if needed
library(readxl)
library(dplyr)
library(lubridate)
library(stringr)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
PATH_Q = file.path("..", "01_project_data", "2_backbone", "questionnaires", "2_2025-09-26_adults.xlsx");
# Read in the data (replace with your file path)
df <- read_excel(PATH_Q)
# Convert start and submit dates to POSIXct (date-time objects)
df <- df %>%
mutate(
startdate = as.POSIXct(startdate, format = "%Y-%m-%d %H:%M:%S"),
submitdate = as.POSIXct(submitdate, format = "%Y-%m-%d %H:%M:%S")
)
# Calculate duration in minutes for each participant
df <- df %>%
mutate(duration_minutes = as.numeric(difftime(submitdate, startdate, units = "mins")))
# Compute the average duration (ignoring NAs)
average_duration <- median(df$duration_minutes, na.rm = TRUE)
# Print result
cat("Average completion time:", round(average_duration, 2), "minutes\n")
### ---- PsyToolkit: average completion time (minutes) ----
# Read PsyToolkit survey export (adjust the path)
PATH_T = file.path("..", "01_project_data", "2_backbone", "experiment_data", "2_2025-09-26_adults.xlsx");
psy <-  read_excel(PATH_T)
# Parse TIME_start / TIME_end that may look like:
# "YYYY-MM-DD HH:MM:SS" or "DD.MM.YYYY HH:MM:SS" (common in EU exports)
parse_dt <- function(x) {
x <- str_trim(as.character(x))
suppressWarnings(parse_date_time(
x,
orders = c("Y-m-d H:M:S","Y-m-d H:M",
"d.m.Y H:M:S","d.m.Y H:M",
"Y/m/d H:M:S","Y/m/d H:M"),
tz = "UTC",
exact = FALSE
))
}
res <- psy %>%
mutate(
start_ts = parse_dt(.data$TIME_start),
end_ts   = parse_dt(.data$TIME_end),
duration_min_raw = as.numeric(difftime(end_ts, start_ts, units = "mins"))
) %>%
# basic quality control: drop negatives, ultra-short blips, and extreme outliers
mutate(duration_min = ifelse(duration_min_raw < 0 | duration_min_raw < 0.1 | duration_min_raw > 8*60,
NA_real_, duration_min_raw))
valid_n <- sum(!is.na(res$duration_min))
if (valid_n == 0) {
cat("No valid durations found. Check that TIME_start / TIME_end parse correctly.\n",
"Example values:\n",
head(psy[c('TIME_start','TIME_end')], 5), "\n")
} else {
avg <- mean(res$duration_min, na.rm = TRUE)
med <- median(res$duration_min, na.rm = TRUE)
cat(sprintf("PsyToolkit – average: %.2f min | median: %.2f min | n=%d valid of %d\n",
avg, med, valid_n, nrow(res)))
K:\Wilken Arbeitsordner\Clinical_Backbone_RU5389\01_project_data\3_backbone\questionnaires}
# Load necessary library
# install.packages(c("readxl","dplyr","lubridate","stringr"))  # if needed
library(readxl)
library(dplyr)
library(lubridate)
library(stringr)
setwd(dirname(rstudioapi::getSourceEditorContext()$path))
PATH_Q = file.path("..", "01_project_data", "2_backbone", "questionnaires", "2_2025-09-26_adults.xlsx");
# Read in the data (replace with your file path)
df <- read_excel(PATH_Q)
# Convert start and submit dates to POSIXct (date-time objects)
df <- df %>%
mutate(
startdate = as.POSIXct(startdate, format = "%Y-%m-%d %H:%M:%S"),
submitdate = as.POSIXct(submitdate, format = "%Y-%m-%d %H:%M:%S")
)
# Calculate duration in minutes for each participant
df <- df %>%
mutate(duration_minutes = as.numeric(difftime(submitdate, startdate, units = "mins")))
# Compute the average duration (ignoring NAs)
average_duration <- median(df$duration_minutes, na.rm = TRUE)
# Print result
cat("Average completion time:", round(average_duration, 2), "minutes\n")
### ---- PsyToolkit: average completion time (minutes) ----
# Read PsyToolkit survey export (adjust the path)
PATH_T = file.path("..", "01_project_data", "2_backbone", "experiment_data", "2_2025-09-26_adults.xlsx");
psy <-  read_excel(PATH_T)
# Parse TIME_start / TIME_end that may look like:
# "YYYY-MM-DD HH:MM:SS" or "DD.MM.YYYY HH:MM:SS" (common in EU exports)
parse_dt <- function(x) {
x <- str_trim(as.character(x))
suppressWarnings(parse_date_time(
x,
orders = c("Y-m-d H:M:S","Y-m-d H:M",
"d.m.Y H:M:S","d.m.Y H:M",
"Y/m/d H:M:S","Y/m/d H:M"),
tz = "UTC",
exact = FALSE
))
}
res <- psy %>%
mutate(
start_ts = parse_dt(.data$TIME_start),
end_ts   = parse_dt(.data$TIME_end),
duration_min_raw = as.numeric(difftime(end_ts, start_ts, units = "mins"))
) %>%
# basic quality control: drop negatives, ultra-short blips, and extreme outliers
mutate(duration_min = ifelse(duration_min_raw < 0 | duration_min_raw < 0.1 | duration_min_raw > 8*60,
NA_real_, duration_min_raw))
valid_n <- sum(!is.na(res$duration_min))
if (valid_n == 0) {
cat("No valid durations found. Check that TIME_start / TIME_end parse correctly.\n",
"Example values:\n",
head(psy[c('TIME_start','TIME_end')], 5), "\n")
} else {
avg <- mean(res$duration_min, na.rm = TRUE)
med <- median(res$duration_min, na.rm = TRUE)
cat(sprintf("PsyToolkit – average: %.2f min | median: %.2f min | n=%d valid of %d\n",
avg, med, valid_n, nrow(res)))
}
# project 3:
