paste0("  • Items expected but NOT found in questionnaire: ",
paste(not_in_q, collapse = ", "), "\n")
else "",
if (length(not_in_ii))
paste0("  • Columns in questionnaire NOT present in Item Information: ",
paste(not_in_ii, collapse = ", "), "\n")
else ""
)
warning(msg, call. = FALSE)
log_msg(msg)
} else {
log_msg("Header check: all questionnaire columns align with Item Information.")
}
# Return invisibly so processing continues
invisible(TRUE)
}
remove_flagged_rows <- function(q_df, sample) {
if (!"rushing_flag" %in% names(q_df)) {
log_msg("Column 'rushing_flag' not found; 0 rows removed.")
return(list(clean = q_df, discarded = tibble()))
}
discarded <- q_df %>% filter(.data$rushing_flag == TRUE)
clean     <- q_df %>% filter(is.na(.data$rushing_flag) | .data$rushing_flag == FALSE)
log_msg(glue::glue("Sample '{sample}': removed {nrow(discarded)} rows due to rushing_flag == TRUE."))
list(clean = clean, discarded = discarded)
}
build_keys <- function(item_info) {
# Nested keys for later analysis/filtering
# Returns a named list with:
# - items_by_scale
# - items_by_subscale
# - items_by_higher_order
# - nested hierarchy
ii <- item_info %>%
mutate(
scale = as.character(.data$scale),
subscale = as.character(.data$subscale),
higher_order_subscale = as.character(.data$higher_order_subscale)
)
items_by_scale <- ii %>% group_by(.data$scale) %>% summarise(items = list(unique(item_norm)), .groups = "drop")
items_by_sub   <- ii %>% group_by(.data$scale, .data$subscale) %>% summarise(items = list(unique(item_norm)), .groups = "drop")
items_by_ho    <- ii %>% group_by(.data$higher_order_subscale) %>% summarise(items = list(unique(item_norm)), .groups = "drop")
nested <- ii %>%
group_by(.data$scale, .data$higher_order_subscale, .data$subscale) %>%
summarise(items = list(unique(item_norm)), .groups = "drop") %>%
group_by(.data$scale, .data$higher_order_subscale) %>%
summarise(subscales = list(tibble(subscale = .data$subscale, items = .data$items)), .groups = "drop") %>%
group_by(.data$scale) %>%
summarise(higher_order = list(tibble(higher_order_subscale = .data$higher_order_subscale, subscales = .data$subscales)), .groups = "drop")
list(
items_by_scale = items_by_scale,
items_by_subscale = items_by_sub,
items_by_higher_order = items_by_ho,
nested = nested
)
}
reverse_code_long <- function(long_df, item_info, scoring) {
# long_df has: id columns + item_norm + value + (optional) p, sample
# Join item_info to get scale & reverse flag, then scoring by scale for min,max
out <- long_df %>%
left_join(select(item_info, item_norm, scale, reverse_coded), by = "item_norm") %>%
left_join(select(scoring, scale, min, max), by = "scale") %>%
mutate(
value_rc = ifelse(isTRUE(reverse_coded),
.data$min + .data$max - as.numeric(.data$value),
as.numeric(.data$value))
) %>%
select(-min, -max)
out
}
pivot_items_long <- function(q_df, item_cols, item_info) {
# Normalize names and pivot
col_map <- tibble(
orig = item_cols,
item_norm = normalize_id(item_cols)
)
q2 <- q_df %>%
mutate(row_id__ = dplyr::row_number()) %>%
pivot_longer(cols = all_of(col_map$orig), names_to = "orig", values_to = "value") %>%
left_join(col_map, by = "orig") %>%
select(-orig)
# Keep only items that exist in item_info (strictness already asserted)
q2 %>% filter(.data$item_norm %in% item_info$item_norm)
}
pivot_items_wide <- function(long_df) {
long_df %>%
select(-row_id__) %>%
pivot_wider(names_from = item_norm, values_from = value_rc)
}
export_per_project <- function(df_clean, df_discard, sample) {
out_dir <- fs::path(DIR_EXPORT, sample)
fs::dir_create(out_dir)
# master exports
readr::write_csv(df_clean,   fs::path(out_dir, glue::glue("{sample}_clean_master.csv")))
readr::write_csv(df_discard, fs::path(out_dir, glue::glue("{sample}_discarded.csv")))
if (!"p" %in% names(df_clean)) {
log_msg("Column 'p' not found — skipping per-project split.")
return(invisible(NULL))
}
df_split <- split(df_clean, df_clean$p)
purrr::iwalk(df_split, function(dd, proj) {
safe_proj <- gsub("[^A-Za-z0-9_-]+", "_", proj)
filepath <- fs::path(out_dir, glue::glue("{sample}_project-{safe_proj}_clean.csv"))
readr::write_csv(dd, filepath)
})
log_msg(glue::glue("Exported {length(df_split)} project-level files for sample '{sample}'."))
}
save_keys <- function(keys, sample) {
path_rds  <- fs::path(DIR_KEYS, glue::glue("{sample}_keys.rds"))
path_json <- fs::path(DIR_KEYS, glue::glue("{sample}_keys.json"))
saveRDS(keys, path_rds)
jsonlite::write_json(keys, path_json, pretty = TRUE, auto_unbox = TRUE)
log_msg(glue::glue("Saved keys for '{sample}' to: {path_rds} and {path_json}"))
}
# ---- One-sample driver -------------------------------------------------------
process_sample <- function(sample,
questionnaire_path = NULL,
iteminfo_path = NULL,
scoring_df = NULL,
drop_criteria_fun = remove_flagged_rows) {
log_msg("\n--- Processing sample: ", sample, " ---")
if (is.null(questionnaire_path)) questionnaire_path <- latest_questionnaire_for_sample(sample)
if (is.na(questionnaire_path)) {
log_msg("No questionnaire file found for sample '", sample, "'. Skipping.")
return(invisible(NULL))
}
if (is.null(iteminfo_path)) iteminfo_path <- latest_iteminfo_for_sample(sample)
if (is.null(scoring_df)) scoring_df <- read_scoring(latest_scoring())
q_raw <- read_questionnaire(questionnaire_path)
ii    <- read_item_info(iteminfo_path)
if (is.null(ii)) {
log_msg("Item Information missing; cannot proceed with mapping and reverse coding. Skipping sample.")
return(invisible(NULL))
}
# Strict header mapping
check_header_match(q_raw, ii)
# Remove flagged rows (extensible criteria)
dropped <- drop_criteria_fun(q_raw, sample)
q_clean0 <- dropped$clean
q_discard <- dropped$discarded
# Identify item columns
parts <- split_id_and_item_columns(q_clean0, item_info_norm_ids = ii$item_norm)
id_cols   <- parts$id_cols
item_cols <- parts$item_cols
# Long → reverse code → wide
q_long <- pivot_items_long(q_clean0, item_cols, ii)
# carry over id columns (including 'p' and any other metadata)
q_long <- q_long %>%
left_join(q_clean0 %>% mutate(row_id__ = dplyr::row_number()) %>% select(row_id__, all_of(id_cols)),
by = "row_id__")
q_long_rc <- reverse_code_long(q_long, ii, scoring_df)
q_wide_rc <- pivot_items_wide(q_long_rc)
# Reorder: id columns first
q_final <- q_wide_rc %>%
select(any_of(id_cols), dplyr::everything())
# Build and save keys
keys <- build_keys(ii)
save_keys(keys, sample)
# Export
export_per_project(q_final, q_discard, sample)
log_msg("--- Done sample: ", sample, " ---\n")
invisible(list(
data_clean = q_final,
data_discarded = q_discard,
keys = keys
))
}
# ---- Main --------------------------------------------------------------------
# Samples available (extend whenever new samples appear)
ALL_SAMPLES <- c("adolescents", "adults", "children_p6", "children_parents", "parents_p6")
# For now, process only adults; later switch to ALL_SAMPLES
SAMPLES_TO_PROCESS <- c("adults")  # change to ALL_SAMPLES when ready
# Preload scoring (once)
SCORING <- read_scoring(latest_scoring())
results <- purrr::map(SAMPLES_TO_PROCESS, ~ process_sample(.x, scoring_df = SCORING))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# FOR: Separate Backbone Data by Project  — Cleaning & Export Pipeline
# Authors: Saskia Wilken (saskia.wilken@uni-hamburg.de, saskia.a.wilken@gmail.com))
# First edited: 2025-11-07 (SW)
#
# Description:
# This script reads questionnaire data (LimeSurvey, PsyToolkit), fixes known
# VP-ID issues, removes test/empty entries, separates data per project, exports
# cleaned datasets (and discarded rows) to disk, and manages logging.
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ---- Setup -------------------------------------------------------------------
ensure_packages <- function(pkgs) {
to_install <- pkgs[!pkgs %in% rownames(installed.packages())]
if (length(to_install)) install.packages(to_install, quiet = TRUE)
invisible(lapply(pkgs, require, character.only = TRUE))
}
ensure_packages(c(
"readxl", "writexl", "janitor", "stringr", "dplyr", "tidyr",
"purrr", "lubridate", "tibble", "glue", "fs", "jsonlite"
))
# ---- Paths & Logging ---------------------------------------------------------
script_dir <- function() {
# Robust-ish script directory detection (Rscript / RStudio / source)
if (!interactive()) {
args <- commandArgs(trailingOnly = FALSE)
file_arg <- "--file="
filepath <- sub(file_arg, "", args[grep(file_arg, args)])
if (length(filepath) == 1) return(normalizePath(dirname(filepath), winslash = "/"))
}
if (requireNamespace("rstudioapi", quietly = TRUE) &&
rstudioapi::isAvailable()) {
p <- tryCatch(rstudioapi::getActiveDocumentContext()$path, error = function(e) "")
if (nzchar(p)) return(normalizePath(dirname(p), winslash = "/"))
}
return(normalizePath(getwd(), winslash = "/"))
}
ROOT <- script_dir()
DIR_QUESTIONNAIRES <- fs::path(ROOT, "01_project_data", "all_projects_backbone", "questionnaires")
DIR_INFO            <- fs::path(ROOT, "information")
DIR_EXPORT          <- fs::path(ROOT, "02_cleaned")
DIR_KEYS            <- fs::path(ROOT, "keys")
DIR_LOGS            <- fs::path(ROOT, "logs")
fs::dir_create(DIR_EXPORT)
fs::dir_create(DIR_KEYS)
fs::dir_create(DIR_LOGS)
logfile <- fs::path(DIR_LOGS, glue::glue("clean_questionnaires_{format(Sys.time(), '%Y-%m-%d_%H%M%S')}.log"))
log_msg <- function(..., .sep = "", .newline = TRUE) {
msg <- paste0("[", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), "] ", paste0(..., collapse = .sep))
if (.newline) msg <- paste0(msg, "\n")
cat(msg)
cat(msg, file = logfile, append = TRUE)
}
# ---- Filename helpers --------------------------------------------------------
normalize_sample_case <- function(sample) {
# For item information files where Sample is Capitalized
stringr::str_replace_all(stringr::str_to_title(sample), "_", " ")
}
# Matchers:
#   Questionnaire: ALL_YYYY-MM-DD_<sample>_questionnaire.xlsx
#   Item Info:     YYYY-MM-DD_Item_Information_<Sample>.xlsx  (Sample Capitalized)
#   Scoring:       YYYY-MM-DD_Scoring.xlsx (single, latest if multiple)
extract_date <- function(x) {
d <- stringr::str_match(x, "(\\d{4}-\\d{2}-\\d{2})")[,2]
suppressWarnings(lubridate::ymd(d))
}
latest_file_by_pattern <- function(dir, pattern) {
# List files first, then filter by basename with regex (not full path)
files_all <- fs::dir_ls(dir, type = "file", fail = FALSE)
files <- files_all[grepl(pattern, basename(files_all))]
if (!length(files)) return(NA_character_)
dts <- extract_date(basename(files))
files <- files[order(dts, decreasing = TRUE)]
files[1]
}
latest_questionnaire_for_sample <- function(sample) {
# Note: data_type fixed to questionnaire
patt <- glue::glue("^ALL_\\d{{4}}-\\d{{2}}-\\d{{2}}_{sample}_questionnaire\\.xlsx$")
latest_file_by_pattern(DIR_QUESTIONNAIRES, patt)
}
latest_iteminfo_for_sample <- function(sample) {
SampleCap <- normalize_sample_case(sample)
patt <- glue::glue("^\\d{{4}}-\\d{{2}}-\\d{{2}}_Item_Information_{SampleCap}\\.xlsx$")
latest_file_by_pattern(DIR_INFO, patt)
}
latest_scoring <- function() {
patt <- "^\\d{4}-\\d{2}-\\d{2}_Scoring\\.xlsx$"
latest_file_by_pattern(DIR_INFO, patt)
}
# ---- Normalization helpers ---------------------------------------------------
# Normalize item IDs so headers match Item Information::Item exactly, even if R renamed them
normalize_id <- function(x) {
x %>%
as.character() %>%
stringr::str_to_lower() %>%
# Keep only letters+digits, drop everything else
stringr::str_replace_all("[^a-z0-9]", "")
}
# ---- IO helpers --------------------------------------------------------------
read_questionnaire <- function(filepath) {
stopifnot(is.character(filepath), length(filepath) == 1, fs::file_exists(filepath))
log_msg("Reading questionnaire: ", filepath)
df <- suppressMessages(readxl::read_excel(filepath))
df <- janitor::remove_empty(df, which = c("rows", "cols"))
tibble::as_tibble(df)
}
read_item_info <- function(filepath) {
if (is.null(filepath) || is.na(filepath)) {
log_msg("Item Information file not found for this sample — continuing without it.")
return(NULL)
}
stopifnot(is.character(filepath), length(filepath) == 1, fs::file_exists(filepath))
log_msg("Reading Item Information: ", filepath)
suppressMessages(readxl::read_excel(filepath)) |>
janitor::clean_names() |>
dplyr::mutate(
item_norm = normalize_id(.data$item),
reverse_coded = as.logical(.data$reverse_coded)
)
}
read_scoring <- function(filepath) {
stopifnot(is.character(filepath), length(filepath) == 1, fs::file_exists(filepath))
log_msg("Reading Scoring file: ", filepath)
suppressMessages(readxl::read_excel(filepath)) |>
janitor::clean_names() |>
dplyr::mutate(dplyr::across(c(min, max), as.numeric))
}
# ---- Core processing ---------------------------------------------------------
split_id_and_item_columns <- function(q_df, item_info) {
# Only keep items that have a non-missing Scale entry
item_info_valid <- item_info %>%
dplyr::filter(!is.na(scale) & scale != "") %>%
dplyr::mutate(item_norm = normalize_id(item))
valid_ids <- unique(item_info_valid$item_norm)
col_norm  <- normalize_id(names(q_df))
is_item   <- col_norm %in% valid_ids
id_cols   <- names(q_df)[!is_item]
item_cols <- names(q_df)[is_item]
log_msg("Using only items with defined 'Scale' in Item Information.")
log_msg("Detected ", length(item_cols), " item columns and ",
length(id_cols), " meta/ID columns for this sample.")
list(
id_cols   = id_cols,
item_cols = item_cols,
item_info_valid = item_info_valid
)
}
check_header_match <- function(q_df, item_info) {
# Normalize both sides
q_norm  <- normalize_id(names(q_df))
ii_norm <- unique(item_info$item_norm)
not_in_q  <- setdiff(ii_norm, q_norm)
not_in_ii <- setdiff(q_norm, ii_norm)
# Exempt obvious non-item identifiers
exempt <- c("p", "project", "rushingflag", "participant",
"participantid", "vp", "vpid")
not_in_ii <- setdiff(not_in_ii, exempt)
# Compose a human-readable message
if (length(not_in_q) || length(not_in_ii)) {
msg <- paste0(
"⚠️  Header / Item Information mismatch detected.\n",
if (length(not_in_q))
paste0("  • Items expected but NOT found in questionnaire: ",
paste(not_in_q, collapse = ", "), "\n")
else "",
if (length(not_in_ii))
paste0("  • Columns in questionnaire NOT present in Item Information: ",
paste(not_in_ii, collapse = ", "), "\n")
else ""
)
warning(msg, call. = FALSE)
log_msg(msg)
} else {
log_msg("Header check: all questionnaire columns align with Item Information.")
}
# Return invisibly so processing continues
invisible(TRUE)
}
remove_flagged_rows <- function(q_df, sample) {
if (!"rushing_flag" %in% names(q_df)) {
log_msg("Column 'rushing_flag' not found; 0 rows removed.")
return(list(clean = q_df, discarded = tibble()))
}
discarded <- q_df %>% filter(.data$rushing_flag == TRUE)
clean     <- q_df %>% filter(is.na(.data$rushing_flag) | .data$rushing_flag == FALSE)
log_msg(glue::glue("Sample '{sample}': removed {nrow(discarded)} rows due to rushing_flag == TRUE."))
list(clean = clean, discarded = discarded)
}
build_keys <- function(item_info) {
# Nested keys for later analysis/filtering
# Returns a named list with:
# - items_by_scale
# - items_by_subscale
# - items_by_higher_order
# - nested hierarchy
ii <- item_info %>%
mutate(
scale = as.character(.data$scale),
subscale = as.character(.data$subscale),
higher_order_subscale = as.character(.data$higher_order_subscale)
)
items_by_scale <- ii %>% group_by(.data$scale) %>% summarise(items = list(unique(item_norm)), .groups = "drop")
items_by_sub   <- ii %>% group_by(.data$scale, .data$subscale) %>% summarise(items = list(unique(item_norm)), .groups = "drop")
items_by_ho    <- ii %>% group_by(.data$higher_order_subscale) %>% summarise(items = list(unique(item_norm)), .groups = "drop")
nested <- ii %>%
group_by(.data$scale, .data$higher_order_subscale, .data$subscale) %>%
summarise(items = list(unique(item_norm)), .groups = "drop") %>%
group_by(.data$scale, .data$higher_order_subscale) %>%
summarise(subscales = list(tibble(subscale = .data$subscale, items = .data$items)), .groups = "drop") %>%
group_by(.data$scale) %>%
summarise(higher_order = list(tibble(higher_order_subscale = .data$higher_order_subscale, subscales = .data$subscales)), .groups = "drop")
list(
items_by_scale = items_by_scale,
items_by_subscale = items_by_sub,
items_by_higher_order = items_by_ho,
nested = nested
)
}
reverse_code_long <- function(long_df, item_info, scoring) {
# long_df has: id columns + item_norm + value + (optional) p, sample
# Join item_info to get scale & reverse flag, then scoring by scale for min,max
out <- long_df %>%
left_join(select(item_info, item_norm, scale, reverse_coded), by = "item_norm") %>%
left_join(select(scoring, scale, min, max), by = "scale") %>%
mutate(
value_rc = ifelse(isTRUE(reverse_coded),
.data$min + .data$max - as.numeric(.data$value),
as.numeric(.data$value))
) %>%
select(-min, -max)
out
}
pivot_items_long <- function(q_df, item_cols, item_info) {
# Normalize names and pivot
col_map <- tibble(
orig = item_cols,
item_norm = normalize_id(item_cols)
)
q2 <- q_df %>%
mutate(row_id__ = dplyr::row_number()) %>%
pivot_longer(cols = all_of(col_map$orig), names_to = "orig", values_to = "value") %>%
left_join(col_map, by = "orig") %>%
select(-orig)
# Keep only items that exist in item_info (strictness already asserted)
q2 %>% filter(.data$item_norm %in% item_info$item_norm)
}
pivot_items_wide <- function(long_df) {
long_df %>%
select(-row_id__) %>%
pivot_wider(names_from = item_norm, values_from = value_rc)
}
export_per_project <- function(df_clean, df_discard, sample) {
out_dir <- fs::path(DIR_EXPORT, sample)
fs::dir_create(out_dir)
# master exports
readr::write_csv(df_clean,   fs::path(out_dir, glue::glue("{sample}_clean_master.csv")))
readr::write_csv(df_discard, fs::path(out_dir, glue::glue("{sample}_discarded.csv")))
if (!"p" %in% names(df_clean)) {
log_msg("Column 'p' not found — skipping per-project split.")
return(invisible(NULL))
}
df_split <- split(df_clean, df_clean$p)
purrr::iwalk(df_split, function(dd, proj) {
safe_proj <- gsub("[^A-Za-z0-9_-]+", "_", proj)
filepath <- fs::path(out_dir, glue::glue("{sample}_project-{safe_proj}_clean.csv"))
readr::write_csv(dd, filepath)
})
log_msg(glue::glue("Exported {length(df_split)} project-level files for sample '{sample}'."))
}
save_keys <- function(keys, sample) {
path_rds  <- fs::path(DIR_KEYS, glue::glue("{sample}_keys.rds"))
path_json <- fs::path(DIR_KEYS, glue::glue("{sample}_keys.json"))
saveRDS(keys, path_rds)
jsonlite::write_json(keys, path_json, pretty = TRUE, auto_unbox = TRUE)
log_msg(glue::glue("Saved keys for '{sample}' to: {path_rds} and {path_json}"))
}
# ---- One-sample driver -------------------------------------------------------
process_sample <- function(sample,
questionnaire_path = NULL,
iteminfo_path = NULL,
scoring_df = NULL,
drop_criteria_fun = remove_flagged_rows) {
log_msg("\n--- Processing sample: ", sample, " ---")
if (is.null(questionnaire_path)) questionnaire_path <- latest_questionnaire_for_sample(sample)
if (is.na(questionnaire_path)) {
log_msg("No questionnaire file found for sample '", sample, "'. Skipping.")
return(invisible(NULL))
}
if (is.null(iteminfo_path)) iteminfo_path <- latest_iteminfo_for_sample(sample)
if (is.null(scoring_df)) scoring_df <- read_scoring(latest_scoring())
q_raw <- read_questionnaire(questionnaire_path)
ii    <- read_item_info(iteminfo_path)
if (is.null(ii)) {
log_msg("Item Information missing; cannot proceed with mapping and reverse coding. Skipping sample.")
return(invisible(NULL))
}
# Strict header mapping
check_header_match(q_raw, ii)
# Remove flagged rows (extensible criteria)
dropped <- drop_criteria_fun(q_raw, sample)
q_clean0 <- dropped$clean
q_discard <- dropped$discarded
# Identify item columns
parts <- split_id_and_item_columns(q_clean0, ii)
id_cols   <- parts$id_cols
item_cols <- parts$item_cols
ii        <- parts$item_info_valid  # restrict to items with defined Scale
# Long → reverse code → wide
q_long <- pivot_items_long(q_clean0, item_cols, ii)
# carry over id columns (including 'p' and any other metadata)
q_long <- q_long %>%
left_join(q_clean0 %>% mutate(row_id__ = dplyr::row_number()) %>% select(row_id__, all_of(id_cols)),
by = "row_id__")
q_long_rc <- reverse_code_long(q_long, ii, scoring_df)
q_wide_rc <- pivot_items_wide(q_long_rc)
# Reorder: id columns first
q_final <- q_wide_rc %>%
select(any_of(id_cols), dplyr::everything())
# Build and save keys
keys <- build_keys(ii)
save_keys(keys, sample)
# Export
export_per_project(q_final, q_discard, sample)
log_msg("--- Done sample: ", sample, " ---\n")
invisible(list(
data_clean = q_final,
data_discarded = q_discard,
keys = keys
))
}
# ---- Main --------------------------------------------------------------------
# Samples available (extend whenever new samples appear)
ALL_SAMPLES <- c("adolescents", "adults", "children_p6", "children_parents", "parents_p6")
# For now, process only adults; later switch to ALL_SAMPLES
SAMPLES_TO_PROCESS <- c("adults")  # change to ALL_SAMPLES when ready
# Preload scoring (once)
SCORING <- read_scoring(latest_scoring())
results <- purrr::map(SAMPLES_TO_PROCESS, ~ process_sample(.x, scoring_df = SCORING))
